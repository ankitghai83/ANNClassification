{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End To End Deep Learning project using the Simple RNN Text Classification   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ankit Ghai\\Documents\\PycharmProjects\\pythonProject\\ANNClassification\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb      # import the built-in imdb dataset in keras\n",
    "from tensorflow.keras.preprocessing import sequence \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN     # import the necessary layers Dense for fully connected layer, Embedding for embedding layer, SimpleRNN for RNN layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:(25000,), Training label shape:(25000,)\n",
      "Testing data shape:(25000,), Testing label shape:(25000,)\n"
     ]
    }
   ],
   "source": [
    "## Load imdb dataset\n",
    "max_features = 10000     # the maximum number of words to consider as features\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)   # load the imdb dataset with the top 10000 most frequent words\n",
    "\n",
    "\n",
    "print(f'Training data shape:{X_train.shape}, Training label shape:{y_test.shape}')   # print the shape of the training and testing data\n",
    "print(f'Testing data shape:{X_test.shape}, Testing label shape:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review (as integers):[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "Sample label:1\n"
     ]
    }
   ],
   "source": [
    "## Inspect sample reviews and its label\n",
    "sample_review = X_train[0]   # get the first review\n",
    "sample_label = y_train[0]    # get the label of the first review\n",
    "\n",
    "print(f'Sample review (as integers):{sample_review}')   # print the first review as integers\n",
    "\n",
    "print(f'Sample label:{sample_label}')   # print the label of the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping word index back to words\n",
    "word_index = imdb.get_word_index()   # get the word index from the imdb dataset\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}   # reverse the word index to get the word from the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded review of the sample review:? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "## Decoded review of sample review( for our understanding)\n",
    "decoded_review=' '.join([reverse_word_index.get(i-3,'?') for i in sample_review])\n",
    "print(f'Decoded review of the sample review:{decoded_review}')   # print the decoded review of the first review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   19,  178,   32],\n",
       "       [   0,    0,    0, ...,   16,  145,   95],\n",
       "       [   0,    0,    0, ...,    7,  129,  113],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4, 3586,    2],\n",
       "       [   0,    0,    0, ...,   12,    9,   23],\n",
       "       [   0,    0,    0, ...,  204,  131,    9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying the padding sequence to the reviews to have same length of each review such that in RNN we can have fixed input size\n",
    "\n",
    "max_len=500   # maximum length of the review\n",
    "\n",
    "X_train=sequence.pad_sequences(X_train,maxlen=max_len)   # pad the training data to have the maximum length of 500\n",
    "X_test=sequence.pad_sequences(X_test,maxlen=max_len)   # pad the testing data to have the maximum length of 500\n",
    "X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ankit Ghai\\Documents\\PycharmProjects\\pythonProject\\ANNClassification\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TRain Simple RNN\n",
    "model=Sequential()   # create a sequential model\n",
    "model.add(Embedding(max_features,128,input_length=max_len))   # add an embedding layer with 128 units\n",
    "model.add(SimpleRNN(128,activation='relu'))  # add a simple RNN layer with 128 units\n",
    "model.add(Dense(1,activation='sigmoid'))   # add a dense layer with 1 unit and sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an instance for EarlyStopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystopping=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)   # create an instance for EarlyStopping callback\n",
    "earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])   # compile the model with adam optimizer, binary_crossentropy loss function and accuracy as the metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Ankit Ghai\\Documents\\PycharmProjects\\pythonProject\\ANNClassification\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Ankit Ghai\\Documents\\PycharmProjects\\pythonProject\\ANNClassification\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "625/625 [==============================] - 84s 130ms/step - loss: 0.5856 - accuracy: 0.6780 - val_loss: 0.5615 - val_accuracy: 0.6936\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 42s 68ms/step - loss: 1959046414336.0000 - accuracy: 0.7452 - val_loss: 0.6376 - val_accuracy: 0.6062\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.3946 - accuracy: 0.8295 - val_loss: 0.4058 - val_accuracy: 0.8292\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 39s 63ms/step - loss: 0.2690 - accuracy: 0.8900 - val_loss: 0.4405 - val_accuracy: 0.8204\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 60s 97ms/step - loss: 0.2034 - accuracy: 0.9218 - val_loss: 0.4144 - val_accuracy: 0.8362\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.1431 - accuracy: 0.9474 - val_loss: 0.4447 - val_accuracy: 0.8486\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.1193 - accuracy: 0.9581 - val_loss: 0.4885 - val_accuracy: 0.8424\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.0920 - accuracy: 0.9672 - val_loss: 0.5033 - val_accuracy: 0.8356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c5215b1f50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the model\n",
    "model.fit(X_train,y_train,epochs=10,batch_size=32,validation_split=0.2,callbacks=[earlystopping])   # train the model with 10 epochs, batch size of 128, validation split of 0.2 and earlystopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankit Ghai\\Documents\\PycharmProjects\\pythonProject\\ANNClassification\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "## save the model file\n",
    "model.save('simple_rnn_imdb.h5')   # save the model to a file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
